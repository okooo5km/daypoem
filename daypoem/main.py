#!/usr/bin/env python3
import os
import re
import csv
import json
import typer
import openai
import requests
import pkg_resources
from pathlib import Path
from bs4 import BeautifulSoup
from dotenv import load_dotenv
from prettytable import PrettyTable
from datetime import datetime, timedelta

load_dotenv()

openai.api_key = os.getenv("OPENAI_API_KEY")
first_day_str = os.getenv("FIRST_DAY", "2023-10-16")
first_day = datetime.strptime(first_day_str, "%Y-%m-%d")
base_dir_str = os.getenv("BASE_DIR", (Path.home() / "Desktop").as_posix())
base_dir = Path(base_dir_str) / "AI ç”»è¯—"

data_path = pkg_resources.resource_filename(__name__, 'data/poems.csv')

summary_prompt = """æˆ‘æ˜¯ä¸€åç”·å£«ï¼Œä½ æ˜¯æˆ‘çš„å°çº¢ä¹¦æ–‡æ¡ˆæ’°å†™åŠ©æ‰‹ï¼Œå¯¹æˆ‘æä¾›çš„ json æ•°æ®æ ¼å¼çš„å¤è¯—è¯å†…å®¹è¿›è¡Œåˆ†æç†è§£ï¼Œæœ€ç»ˆä¼šæ€»ç»“ä¸€å¥è¯åˆ†äº«ï¼Œä½ éœ€è¦æŠŠè‡ªå·±å½“åšè¯»è€…æå‡ºåˆ‡é¢˜ä¸”å€¼å¾—æ€è€ƒçš„é—®é¢˜ï¼ˆä¹Ÿå°±æ˜¯ç¬¬ä¸€äººç§°ï¼‰ï¼Œå°½é‡æ˜¯éå¸¸ç®€çŸ­å’Œå¯Œæœ‰æ–‡é‡‡çš„ä¸€å¥è¯ï¼åŒæ—¶ç”Ÿæˆä¸€ä¸ªä»¥â€œAIç”»è¯—ã€Š{è¯—çš„é¢˜ç›®}ã€‹å¼€å¤´çš„æ–‡æ¡ˆæ ‡é¢˜ï¼Œå°çº¢ä¹¦æ ‡é¢˜ä¸èƒ½è¶…è¿‡ 20 ä¸ªä¸­æ–‡æ±‰å­—ï¼Œè¦æ±‚å¸äººçœ¼çƒä¸”ç¬¦åˆå°çº¢ä¹¦æ–‡æ¡ˆé£æ ¼ï¼Œå¦‚æœå¯èƒ½çš„è¯ä¸€å¥è¯å’Œæ ‡é¢˜ä¸­ä¹Ÿé…ä¸Šè´´åˆ‡çš„ emojiï¼ŒåŒæ—¶å‡ç»ƒå‡ºå¯ä»¥ä½¿ç”¨çš„è¯é¢˜å…³é”®è¯ï¼Œæ¯”å¦‚#å”è¯— #æœˆäº® #æ€å¿µ #çˆ±æƒ… ...ï¼›å¦å¤–æ ¹æ®æˆ‘æä¾›çš„èµæå†…å®¹ï¼Œå†™ä¸€æ®µè´´åˆåŸèµæå†…å®¹ä¸”å¯Œæœ‰æ–‡é‡‡å’Œå“²ç†çš„èµæã€‚ç»¼ä¸Šï¼Œæ•´ä½“è¾“å‡ºçš„å†…å®¹ä¸º json å­—ç¬¦ä¸²æœ¬èº«å°±å¥½ï¼Œä¸è¦ä½¿ç”¨ markdown è¯­æ³•ï¼Œå½¢å¦‚:
{
    "æ ‡é¢˜": "",
    "ä¸€å¥è¯": "",
    "èµæ": "",
    "è¯é¢˜": ""
}"""


def parse_poem_info(url):

    html_content = ""

    poem_id = re.search(r"shiwenv_(\w+)\.aspx", url).group(1)

    # å‘é€ HTTP è¯·æ±‚
    response = requests.get(url)

    # æ£€æŸ¥è¯·æ±‚æ˜¯å¦æˆåŠŸ
    if response.status_code == 200:
        # è¿”å›ç½‘é¡µçš„ HTML å†…å®¹
        html_content = response.text
    else:
        print(
            f"Failed to fetch the webpage. Status code: {response.status_code}")
        return None

    # Initialize BeautifulSoup object
    soup = BeautifulSoup(html_content, 'html.parser')

    # Initialize the JSON object to hold the data
    poem_data = {}

    # Extract poem content
    zhengwen_id = f"zhengwen{poem_id}"
    zhengwen_div = soup.find('div', {'id': zhengwen_id})

    # Title, Author and Content
    poem_data["é¢˜ç›®"] = zhengwen_div.h1.text.strip()
    author_info = zhengwen_div.p.text.strip().split("ã€”")
    poem_data["ä½œè€…"] = {
        "å§“å": author_info[0],
        "æœä»£": author_info[1].rstrip("ã€•"),
    }
    poem_data["è¯—å¥"] = zhengwen_div.find('div').text.strip()

    # Translation and Notes
    yiwen_div = soup.find('span', string="è¯‘æ–‡åŠæ³¨é‡Š")
    if not yiwen_div:
        yiwen_div = soup.find('span', string="æ³¨è§£åŠè¯‘æ–‡")

    if not yiwen_div:
        poem_data["è¯‘æ–‡"] = ""
        poem_data["æ³¨é‡Š"] = ""
    else:
        yishang_div = yiwen_div.find_parent(
            'div', {'class': 'contyishang'})
        yishang_ps = yishang_div.find_all('p')
        poem_data["è¯‘æ–‡"] = yishang_ps[0].text.strip().replace("è¯‘æ–‡", "")
        if len(yishang_ps) > 1:
            poem_data["æ³¨é‡Š"] = yishang_ps[1].text.strip()
        else:
            poem_data["æ³¨é‡Š"] = ""
        if poem_data["æ³¨é‡Š"].endswith("å±•å¼€é˜…è¯»å…¨æ–‡ âˆ¨"):
            poem_data["æ³¨é‡Š"] = poem_data["æ³¨é‡Š"][:poem_data["æ³¨é‡Š"].rfind("ã€‚")+1]

    # Appreciation
    shangxi_span = soup.find('span', string="èµæ")
    if not shangxi_span:
        shangxi_span = soup.find('span', string="è¯„æ")
    if shangxi_span:
        shangxi_div = shangxi_span.find_parent('div')
        shangxi_content = [p.text.strip()
                           for p in shangxi_div.find_next_siblings('p')]
        poem_data["èµæ"] = "\n".join(
            shangxi_content)
        if poem_data["èµæ"].endswith("å±•å¼€é˜…è¯»å…¨æ–‡ âˆ¨"):
            poem_data["èµæ"] = poem_data["èµæ"][:poem_data["èµæ"].rfind("ã€‚")+1]

    # Creation background
    chuangzuo_span = soup.find('span', string="åˆ›ä½œèƒŒæ™¯")
    if chuangzuo_span:
        chuangzuo_div = chuangzuo_span.find_parent('div')
        chuangzuo_content = [p.text.strip()
                             for p in chuangzuo_div.find_next_siblings('p')]
        poem_data["åˆ›ä½œèƒŒæ™¯"] = "\n".join(chuangzuo_content)

    # Author details
    poem_data["ä½œè€…"]["ä»‹ç»"] = ""
    zuozhe_parent_div = soup.find('div', {'class': 'sonspic'})
    if zuozhe_parent_div:
        zuozhe_div = zuozhe_parent_div.find(
            'div', {'class': 'cont'})
        for p in zuozhe_div.find_all('p'):
            poem_data["ä½œè€…"]["ä»‹ç»"] += p.text.strip()
        # åˆ é™¤å½¢å¦‚ "â–º 439ç¯‡è¯—æ–‡ã€€â–º 585æ¡åå¥" çš„å­—ç¬¦ä¸²
        poem_data["ä½œè€…"]["ä»‹ç»"] = re.sub(
            r"â–º \d+ç¯‡è¯—æ–‡ã€€â–º \d+æ¡åå¥", "", poem_data["ä½œè€…"]["ä»‹ç»"])

    return poem_data


def get_poems() -> list:

    poems = []

    with open(data_path, "r", encoding="utf-8") as csvfile:
        reader = csv.reader(csvfile)
        next(reader)  # è·³è¿‡æ ‡é¢˜è¡Œ
        for row in reader:
            poem = {
                "åºå·": row[0],
                "é¢˜ç›®": row[1],
                "ä½œè€…": row[2],
                "ç±»å‹": row[3],
                "é“¾æ¥": row[4]
            }
            poems.append(poem)

    return poems


def get_poem(offset=0):
    poems = get_poems()
    index = (datetime.today() - first_day).days + offset
    return poems[index]


def generate_xhs_note_with_emoji(poem_info):
    template = """{å°çº¢ä¹¦[æ ‡é¢˜]}

ğŸ“œã€Š{é¢˜ç›®}ã€‹ by {ä½œè€…[å§“å]} - {ä½œè€…[æœä»£]}ğŸŒŸ
    
ğŸƒ è¯—å¥ ğŸƒ
{è¯—å¥}

ğŸ“š è¯‘æ–‡ ğŸ“š
{è¯‘æ–‡}

ğŸ” èµæ ğŸ”
{å°çº¢ä¹¦[èµæ]}

ğŸ’¬ ä¸€è¨€ ğŸ’¬
{å°çº¢ä¹¦[ä¸€å¥è¯]}

#AI #AIç”»è¯— #AIç»˜ç”» #AIGC #æ°´å¢¨ç”» {å°çº¢ä¹¦[è¯é¢˜]}
"""

    # å¯¹äºå¤šè¡Œæ–‡æœ¬è¿›è¡Œç¼©è¿›å¤„ç†
    for key in ['è¯‘æ–‡', 'æ³¨é‡Š']:
        poem_info[key] = poem_info[key].replace('\n', '\n  ')

    return template.format(**poem_info)


def daypoem(
    url: str = typer.Option(None, help="The URL of the poem to scrape."),
    xhs: bool = typer.Option(False, help="Generate an xiaohongshu note"),
    offset: int = typer.Option(
        0, help="The number of days to offset from today."),
    list: bool = typer.Option(False, help="List all diary information."),
):
    if list:
        table = PrettyTable()
        table.field_names = ["åºå·", "é¢˜ç›®", "ä½œè€…", "ç±»å‹", "é“¾æ¥"]
        for poem in get_poems():
            table.add_row(
                [poem['åºå·'], poem['é¢˜ç›®'], poem['ä½œè€…'], poem['ç±»å‹'], poem['é“¾æ¥']])
        print(table)
        return

    if not url:
        url = get_poem(offset=offset)["é“¾æ¥"]

    poem_info = parse_poem_info(url)

    xhs_content = ""
    if poem_info:
        print()
        print(json.dumps(poem_info, ensure_ascii=False, indent=4))

        if xhs:
            print("\nğŸš€ summarying ... \n")
            completion = openai.ChatCompletion.create(
                model="gpt-4-1106-preview",
                messages=[
                    {"role": "system", "content": summary_prompt},
                    {"role": "user", "content": json.dumps(
                        poem_info, ensure_ascii=False)}
                ]
            )

            xhs_dict = {
                "æ ‡é¢˜": "",
                "ä¸€å¥è¯": "",
                "èµæ": "",
                "è¯é¢˜": ""
            }
            content_json = json.loads(completion.choices[0].message.content)
            if content_json:
                xhs_dict.update(content_json)

            poem_info["å°çº¢ä¹¦"] = xhs_dict
            xhs_content = generate_xhs_note_with_emoji(poem_info)

            print(xhs_content)

            poem_info["å°çº¢ä¹¦"]["æ–‡æ¡ˆ"] = xhs_content

        min_days_offset = (first_day - datetime.today()).days
        days_offset = offset if offset >= min_days_offset else min_days_offset
        poem_date = datetime.today() + timedelta(days=days_offset)

        date_str = poem_date.strftime("%Y%m%d")
        dir_name = f'{date_str}ã€Š{poem_info["é¢˜ç›®"]}ã€‹'
        dir_path = base_dir / dir_name
        if not dir_path.exists():
            dir_path.mkdir()
        file_path = dir_path / "å°çº¢ä¹¦.txt"
        with file_path.open("w", encoding="utf-8") as f:
            f.write(xhs_content)
        poem_info_file_path = dir_path / "poem_info.json"
        json.dump(poem_info, poem_info_file_path.open(
            "w", encoding="utf-8"), indent=4, ensure_ascii=False)

        # æ‰“å¼€æ–‡ä»¶å¤¹
        os.system(f"open {dir_path.as_uri()}")
        # poem_info echo åˆ°ç³»ç»Ÿå‰ªåˆ‡æ¿
        pretty_poem_info = json.dumps(poem_info, ensure_ascii=False, indent=4)
        os.system(f"echo '{pretty_poem_info}' | pbcopy")

    else:
        print("Failed to fetch the webpage.")


def main():
    typer.run(daypoem)


if __name__ == "__main__":
    main()
